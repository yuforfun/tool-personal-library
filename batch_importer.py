# ä¿®æ­£ [batch_importer.py] å€å¡Š G: å¯¦ä½œæš´åŠ›é˜²å‘†èˆ‡æ‹¬è™Ÿæ¸…æ´—
# ä¿®æ­£åŸå› ï¼šé€éæ­£å‰‡è¡¨é”å¼ç§»é™¤æ›¸åä¸­çš„æ‹¬è™Ÿèˆ‡å‚™è¨»å…§å®¹ï¼Œç¢ºä¿æ¯”å°æ™‚åªé‡å°æ ¸å¿ƒæ›¸åï¼Œé˜²æ­¢é‡è¤‡åŒ¯å…¥ã€‚
# æ›¿æ›/æ–°å¢æŒ‡ç¤ºï¼šè«‹å®Œå…¨è¦†è“‹ batch_importer.pyã€‚

import os
import time
import pandas as pd
import uuid
import random
import re
from datetime import datetime, date, timedelta
from difflib import SequenceMatcher
from opencc import OpenCC
from dataclasses import dataclass
from typing import Optional, List

from modules import scraper, ai_agent, database
from modules.scraper import RawBookData
from modules.models import Book, BookStatus

cc = OpenCC('s2twp')

# ==========================================
# 0. ä½¿ç”¨è€…è¨­å®š
# ==========================================
DATE_STRATEGY = "NONE" 
DEFAULT_RATING_SOURCE_B = 0 

# ==========================================
# 1. è³‡æ–™çµæ§‹èˆ‡æ¯”å°
# ==========================================
@dataclass
class CsvBookCandidate:
    title: str
    author: str
    url: str
    description_text: str
    user_rating: int
    status: BookStatus
    tags: List[str]
    original_source: str
    completed_date: Optional[date] = None

def normalize_text(text: str, aggressive=False) -> str:
    """
    æ­£è¦åŒ–å­—ä¸²
    aggressive=True: æš´åŠ›æ¨¡å¼ï¼Œç§»é™¤æ‰€æœ‰æ‹¬è™ŸåŠå…¶å…§å®¹ (ex: "æ›¸å(å…¨)" -> "æ›¸å")
    """
    if not text: return ""
    text = cc.convert(str(text))
    
    if aggressive:
        # ç§»é™¤æ‹¬è™Ÿå…§çš„å…§å®¹ (åŒ…å«æ‹¬è™Ÿæœ¬èº«)
        # æ”¯æ´: (), [], {}, ã€ã€‘, ï¼ˆï¼‰
        text = re.sub(r'[\(\[\{ã€ï¼ˆ].*?[\)\]\}ã€‘ï¼‰]', '', text)
        # ç§»é™¤å¸¸è¦‹å¾Œç¶´
        text = re.sub(r'(å…¨æ–‡å®Œ|å®Œçµ|é€£è¼‰ä¸­|ç•ªå¤–)', '', text)

    # ç§»é™¤æ¨™é»èˆ‡ç©ºç™½
    for char in [" ", "ã€€", "ï¼Œ", ",", "ã€Š", "ã€‹", "ã€", "ã€‘", "[", "]", "ã€Œ", "ã€", ":", "ï¼š"]:
        text = text.replace(char, "")
        
    return text.lower()

def to_traditional(text: str) -> str:
    if not text: return ""
    return cc.convert(str(text))

def verify_identity(csv_book: CsvBookCandidate, scraped_data: RawBookData) -> tuple[bool, str]:
    # ä½œè€…æ¯”å°
    csv_auth_norm = normalize_text(csv_book.author)
    web_auth_norm = normalize_text(scraped_data.author)
    
    if csv_auth_norm and web_auth_norm and "æœªçŸ¥" not in csv_auth_norm and "æœªçŸ¥" not in web_auth_norm:
        if csv_auth_norm != web_auth_norm:
            if csv_auth_norm not in web_auth_norm and web_auth_norm not in csv_auth_norm:
                return False, f"ä½œè€…ä¸ç¬¦ (CSV: {csv_book.author} vs Web: {scraped_data.author})"

    # æ¨™é¡Œæ¯”å° (é–‹å•Ÿæš´åŠ›æ¸…æ´—æ¨¡å¼)
    csv_title_norm = normalize_text(csv_book.title, aggressive=True)
    web_title_norm = normalize_text(scraped_data.title, aggressive=True)
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    similarity = SequenceMatcher(None, csv_title_norm, web_title_norm).ratio()
    
    # æé«˜é–¥å€¼åˆ° 0.7ï¼Œå› ç‚ºå·²ç¶“åšäº†æš´åŠ›æ¸…æ´—ï¼Œç†è«–ä¸Šè¦å¾ˆåƒ
    if similarity < 0.7 and (csv_title_norm not in web_title_norm) and (web_title_norm not in csv_title_norm): 
        return False, f"æ¨™é¡Œå·®ç•°éå¤§ ({csv_title_norm} vs {web_title_norm}, sim={similarity:.2f})"
        
    return True, "èº«ä»½é©—è­‰é€šé"

# ==========================================
# 2. CSV è®€å–å™¨
# ==========================================
def get_legacy_date():
    if DATE_STRATEGY == "FIXED": return date(2025, 12, 31)
    elif DATE_STRATEGY == "RANDOM":
        start = date(2025, 1, 1)
        return start + timedelta(days=random.randrange((date(2025, 12, 31) - start).days))
    else: return None

def load_source_a_gaming(file_path: str) -> List[CsvBookCandidate]:
    try: df = pd.read_csv(file_path, encoding='utf-8')
    except: df = pd.read_csv(file_path, encoding='big5')
    candidates = []
    for _, row in df.iterrows():
        url = str(row.get('Link', ''))
        if url == 'nan': url = ""
        tag_raw = str(row.get('å‚™è¨»', ''))
        tags = [tag_raw.replace('ã€', '').replace('ã€‘', '').strip()] if (tag_raw and tag_raw != 'nan') else []
        candidates.append(CsvBookCandidate(
            title=str(row.get('æ›¸å', 'æœªçŸ¥æ¨™é¡Œ')).strip(),
            author=str(row.get('ä½œè€…', 'æœªçŸ¥ä½œè€…')).strip(),
            url=url,
            description_text=str(row.get('è©•è«–', '')) if str(row.get('è©•è«–', '')) != 'nan' else "",
            user_rating=str(row.get('æ¨è–¦åº¦', '')).count('â˜…'),
            status=BookStatus.COMPLETED,
            tags=tags,
            original_source="Gaming_CSV",
            completed_date=get_legacy_date()
        ))
    return candidates

def load_source_b_booklist(file_path: str) -> List[CsvBookCandidate]:
    try: df = pd.read_csv(file_path, encoding='utf-8')
    except: df = pd.read_csv(file_path, encoding='big5')
    candidates = []
    for _, row in df.iterrows():
        c_date = None
        d_str = str(row.get('æ—¥æœŸ', ''))
        if d_str and d_str != 'nan':
            try: c_date = datetime.strptime(d_str.strip(), '%Y/%m/%d').date()
            except: pass
        
        status = BookStatus.COMPLETED if (c_date or 'å®Œ' in str(row.get('ç‹€æ…‹', ''))) else BookStatus.UNREAD
        if status == BookStatus.COMPLETED and not c_date: c_date = get_legacy_date()

        tags = []
        t_col = str(row.get('é¡åˆ¥Tag', ''))
        if t_col and t_col != 'nan':
            tags = [t.strip().replace('ï¼ˆ', '(').replace('ï¼‰', ')') for t in t_col.replace('ï¼Œ', ',').split(',') if t.strip()]

        url = str(row.get('ä¾†æº', ''))
        if url == 'nan': url = ""
        candidates.append(CsvBookCandidate(
            title=str(row.get('æ›¸å', 'æœªçŸ¥æ¨™é¡Œ')).strip(),
            author=str(row.get('ä½œè€…', 'æœªçŸ¥ä½œè€…')).strip(),
            url=url,
            description_text=str(row.get('æ–‡æ¡ˆ', '')) if str(row.get('æ–‡æ¡ˆ', '')) != 'nan' else "",
            user_rating=DEFAULT_RATING_SOURCE_B,
            status=status,
            tags=tags,
            original_source="Booklist_CSV",
            completed_date=c_date
        ))
    return candidates

# ==========================================
# 3. ä¸»åŒ¯å…¥é‚è¼¯
# ==========================================
def process_candidate(candidate: CsvBookCandidate, existing_books: dict, report_list: list):
    print(f"\nğŸ“˜ æ­£åœ¨è™•ç†ï¼š{candidate.title} / {candidate.author}")
    
    # --- 1. æš´åŠ›é‡è¤‡æª¢æŸ¥ ---
    cand_key_norm = f"{normalize_text(candidate.title, aggressive=True)}_{normalize_text(candidate.author)}"
    
    for b in existing_books.values():
        # å¦‚æœç¶²å€å®Œå…¨ä¸€æ¨£ -> æ“‹
        if candidate.url and "http" in candidate.url and b.url == candidate.url:
            print(f"   â­ï¸ è·³éï¼šç¶²å€å·²å­˜åœ¨")
            return "SKIPPED_URL_EXIST"
            
        # å¦‚æœ æ›¸å+ä½œè€… (ç¶“éæ¸…æ´—) ä¸€æ¨£ -> æ“‹
        db_key_norm = f"{normalize_text(b.title, aggressive=True)}_{normalize_text(b.author)}"
        if cand_key_norm == db_key_norm:
             print(f"   â­ï¸ è·³éï¼šæ›¸åèˆ‡ä½œè€…å·²å­˜åœ¨ ({b.title})")
             return "SKIPPED_TITLE_EXIST"

    # --- 2. çˆ¬å–èˆ‡é©—è­‰ ---
    scraped_data = None
    verification_passed = False
    failure_reason = None
    is_egg_blog = "egg19910707" in candidate.url or "blog.fc2.com" in candidate.url
    
    if candidate.url and "http" in candidate.url and "drive.google" not in candidate.url:
        try:
            print(f"   ğŸ•·ï¸ å˜—è©¦çˆ¬å–ï¼š{candidate.url[:40]}...")
            scraped_data = scraper.scrape_book(candidate.url)
            if scraped_data:
                passed, msg = verify_identity(candidate, scraped_data)
                if passed:
                    print(f"   âœ… é©—è­‰æˆåŠŸ")
                    verification_passed = True
                else:
                    print(f"   âš ï¸ é©—è­‰å¤±æ•—ï¼š{msg}")
                    failure_reason = f"èº«ä»½ä¸ç¬¦: {msg}"
                    verification_passed = False
            else:
                 print("   âš ï¸ çˆ¬èŸ²å›å‚³ç©ºå€¼")
                 failure_reason = "ç¶²ç«™ç„¡æ³•é€£ç·š"
        except Exception as e:
            print(f"   âš ï¸ çˆ¬å–ç•°å¸¸ ({e})")
            failure_reason = f"çˆ¬èŸ²éŒ¯èª¤: {e}"
    elif candidate.url and "drive.google" in candidate.url:
        failure_reason = "Google Drive"

    if candidate.url and failure_reason and "http" in candidate.url and not is_egg_blog:
        report_list.append({
            "æ›¸å": candidate.title,
            "åŸå§‹ç¶²å€": candidate.url,
            "å¤±æ•—åŸå› ": failure_reason
        })

    # --- 3. è³‡æ–™æº–å‚™ ---
    final_title = candidate.title
    final_author = candidate.author
    final_desc = ""
    final_url = ""
    final_source_name = "CSVåŒ¯å…¥"
    
    if verification_passed and scraped_data:
        final_title = scraped_data.title
        final_author = scraped_data.author
        final_desc = scraped_data.description
        final_url = scraped_data.url
        final_source_name = scraped_data.source_name
    else:
        # é™ç´šä¿è­·
        if is_egg_blog:
            final_url = candidate.url
            final_source_name = "Egg (ä¿ç•™)"
        else:
            final_url = ""
        
        if candidate.original_source == "Booklist_CSV":
            final_desc = candidate.description_text
    
    final_user_review = ""
    if candidate.original_source == "Gaming_CSV":
        final_user_review = candidate.description_text

    # --- 4. AI æ±ºç­– ---
    ai_prompt_desc = final_desc
    if final_user_review: ai_prompt_desc += f"\n{final_user_review}"
    
    ai_result = None
    if len(ai_prompt_desc) > 20: 
        raw_data_for_ai = RawBookData(title=final_title, author=final_author, description=ai_prompt_desc, source_name=final_source_name, url=final_url)
        try:
            time.sleep(1.0)
            ai_result = ai_agent.analyze_book(raw_data_for_ai)
        except: pass
    else:
        print("   ğŸ›‘ è³‡è¨Šä¸è¶³ï¼Œè·³é AI åˆ†æ")

    final_tags = list(set(candidate.tags + (ai_result.tags if ai_result else [])))
    final_tags = [to_traditional(t) for t in final_tags]

    final_date = candidate.completed_date
    if DATE_STRATEGY == "NONE" and final_date is None:
        final_date = None 

    new_book = Book(
        id=str(uuid.uuid4()),
        title=to_traditional(final_title),
        author=to_traditional(final_author),
        source=to_traditional(final_source_name),
        url=final_url,
        status=candidate.status,
        tags=final_tags,
        ai_summary=to_traditional(ai_result.summary) if ai_result else "å¾…è£œå®Œ (è«‹é»æ“Šé‡æ–°åˆ†æ)",
        official_desc=to_traditional(final_desc),
        ai_plot_analysis=to_traditional(ai_result.plot) if ai_result else "è³‡è¨Šä¸è¶³ï¼ŒAI æš«æœªåˆ†æ",
        added_date=date.today(),
        completed_date=final_date,
        user_rating=candidate.user_rating,
        user_review=to_traditional(final_user_review)
    )
    
    try:
        database.insert_book(new_book)
        print(f"   ğŸ’¾ å…¥åº«æˆåŠŸï¼(ID: {new_book.id[:6]})")
        return "SUCCESS"
    except Exception: return "DB_ERROR"

def main():
    database.init_db()
    existing_books = {b.id: b for b in database.get_all_books()}
    candidates = []
    if os.path.exists("source_a.csv"): candidates.extend(load_source_a_gaming("source_a.csv"))
    if os.path.exists("source_b.csv"): candidates.extend(load_source_b_booklist("source_b.csv"))

    if not candidates:
        print("âš ï¸ æ‰¾ä¸åˆ°ä¾†æº CSV")
        return

    failure_report = []
    print(f"ğŸ“Š é–‹å§‹åŒ¯å…¥ {len(candidates)} ç­†è³‡æ–™...")
    stats = {"SUCCESS": 0, "SKIPPED": 0, "ERROR": 0}
    
    for i, cand in enumerate(candidates):
        try:
            result = process_candidate(cand, existing_books, failure_report)
            if "SKIPPED" in result: stats["SKIPPED"] += 1
            elif result == "SUCCESS": stats["SUCCESS"] += 1
            else: stats["ERROR"] += 1
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            stats["ERROR"] += 1
            
    if failure_report:
        pd.DataFrame(failure_report).to_csv("import_failures.csv", index=False, encoding="utf-8-sig")

if __name__ == "__main__":
    main()